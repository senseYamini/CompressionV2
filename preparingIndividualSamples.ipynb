{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"../../Data/empty1_timedomain.csv\",\n",
    "    \"../../Data/jumpAyon_timedomain.csv\",\n",
    "    \"../../Data/situpAyon_timedomain.csv\",\n",
    "    \"../../Data/standAyon_timedomain.csv\",\n",
    "    \"../../Data/walkAyon_timedomain.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "counter = 0\n",
    "for file_path in files:\n",
    "    f = pd.read_csv(file_path, header=None, skiprows=1)\n",
    "    for i in range(0, len(f), 50):\n",
    "        data_chunk = f.iloc[i:i+50].to_numpy()\n",
    "        if len(data_chunk) < 50:  # Check if the chunk is complete\n",
    "            continue \n",
    "        if(np.isnan(data_chunk).any()):\n",
    "            continue\n",
    "        data.append(data_chunk)\n",
    "        labels.append(counter)\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chunk in enumerate(data):\n",
    "    if(chunk.shape[0] != 50 or chunk.shape[1] != 56):\n",
    "        print(f\"inconsistency found for element: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = np.array(data)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_array, labels_array, \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in data: False\n",
      "Inf in data: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for NaN or Inf in your dataset\n",
    "print(\"NaN in data:\", np.isnan(X_train).any())\n",
    "print(\"Inf in data:\", np.isinf(X_train).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UT_HAR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UT_HAR_MLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(50 * 56, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 50 * 56)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UT_HAR_LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UT_HAR_LeNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input size: (1, 50, 56)\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=(3, 1)),  # Output: (32, 16, 52)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2),                                # Output: (32, 8, 26)\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)),  # Output: (64, 4, 13)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, padding=(0, 1)),                # Output: (64, 2, 7)\n",
    "            nn.Conv2d(64, 96, kernel_size=(2, 2), stride=1, padding=(0, 0)),       # Output: (96, 1, 6)\n",
    "            nn.ReLU(True)                       \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(96*1*6, 128),       \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 5)                      \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        # print(f\"Shape after encoder: {x.shape}\")  # Debugging shape\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        # print(f\"Shape after flattening: {x.shape}\")  # Debugging shape after flattening\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = UT_HAR_MLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = UT_HAR_LeNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the hyperparameters\n",
    "batch_size = 1\n",
    "input_size = X_train.shape[1:]\n",
    "hidden_size1 = 500\n",
    "hidden_size2 = 300\n",
    "hidden_size3 = 100\n",
    "num_classes = 5\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "n_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69227/3793950619.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train, y_train = torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype = torch.long).to(device)\n",
      "/tmp/ipykernel_69227/3793950619.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test, y_test = torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "#transfering training, testing data and labels to device\n",
    "X_train, y_train = torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype = torch.long).to(device)\n",
    "X_test, y_test = torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 2/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 3/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 4/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 5/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 6/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 7/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 8/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 9/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 10/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 11/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 12/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 13/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 14/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 15/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 16/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 17/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 18/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 19/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 20/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 21/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 22/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 23/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 24/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 25/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 26/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 27/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 28/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 29/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 30/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 31/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 32/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 33/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 34/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 35/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 36/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 37/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 38/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 39/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 40/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 41/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 42/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 43/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 44/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 45/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 46/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 47/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 48/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 49/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n",
      "Epoch 50/50, Train Loss: 3.7702, Val Loss: 3.9391, Accuracy: 19.15%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for back_idx, (data, label) in enumerate(train_dataloader):\n",
    "        # print(f'data loader data shape: {data.shape}')\n",
    "        #zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        #take the output\n",
    "        output = model1(data)\n",
    "        # output = model2(data)\n",
    "        #find the loss\n",
    "        loss = criterion(output, label)\n",
    "        #back propagation\n",
    "        loss.backward()\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss/len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    # break\n",
    "\n",
    "\n",
    "    model1.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # data = data.unsqueeze(1)\n",
    "            output = model1(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            val_loss += criterion(output, target.long()).item()\n",
    "        avg_val_loss = val_loss / len(test_dataloader)\n",
    "        test_losses.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {100*correct/total:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 1.5046, Val Loss: 1.3731, Accuracy: 35.26%\n",
      "Epoch 2/50, Train Loss: 1.3702, Val Loss: 1.3118, Accuracy: 36.78%\n",
      "Epoch 3/50, Train Loss: 1.3263, Val Loss: 1.2575, Accuracy: 58.66%\n",
      "Epoch 4/50, Train Loss: 1.2588, Val Loss: 1.1403, Accuracy: 61.40%\n",
      "Epoch 5/50, Train Loss: 1.2129, Val Loss: 1.1229, Accuracy: 58.66%\n",
      "Epoch 6/50, Train Loss: 1.1532, Val Loss: 1.0410, Accuracy: 62.92%\n",
      "Epoch 7/50, Train Loss: 1.0957, Val Loss: 1.0757, Accuracy: 59.57%\n",
      "Epoch 8/50, Train Loss: 1.0905, Val Loss: 0.9613, Accuracy: 62.61%\n",
      "Epoch 9/50, Train Loss: 1.0289, Val Loss: 1.0134, Accuracy: 58.97%\n",
      "Epoch 10/50, Train Loss: 1.0037, Val Loss: 0.9478, Accuracy: 66.57%\n",
      "Epoch 11/50, Train Loss: 0.9567, Val Loss: 1.0352, Accuracy: 61.70%\n",
      "Epoch 12/50, Train Loss: 0.9909, Val Loss: 0.9485, Accuracy: 61.09%\n",
      "Epoch 13/50, Train Loss: 0.9656, Val Loss: 0.9110, Accuracy: 62.61%\n",
      "Epoch 14/50, Train Loss: 0.9341, Val Loss: 0.8548, Accuracy: 67.17%\n",
      "Epoch 15/50, Train Loss: 0.9079, Val Loss: 0.8583, Accuracy: 64.13%\n",
      "Epoch 16/50, Train Loss: 0.8818, Val Loss: 0.8722, Accuracy: 66.26%\n",
      "Epoch 17/50, Train Loss: 0.8378, Val Loss: 0.8461, Accuracy: 67.48%\n",
      "Epoch 18/50, Train Loss: 0.8401, Val Loss: 0.9655, Accuracy: 62.31%\n",
      "Epoch 19/50, Train Loss: 0.8249, Val Loss: 0.8019, Accuracy: 68.39%\n",
      "Epoch 20/50, Train Loss: 0.7932, Val Loss: 0.7970, Accuracy: 69.00%\n",
      "Epoch 21/50, Train Loss: 0.7811, Val Loss: 0.8378, Accuracy: 67.78%\n",
      "Epoch 22/50, Train Loss: 0.7447, Val Loss: 0.7509, Accuracy: 69.30%\n",
      "Epoch 23/50, Train Loss: 0.7230, Val Loss: 0.6931, Accuracy: 71.73%\n",
      "Epoch 24/50, Train Loss: 0.7124, Val Loss: 0.7104, Accuracy: 71.43%\n",
      "Epoch 25/50, Train Loss: 0.6942, Val Loss: 0.7153, Accuracy: 70.52%\n",
      "Epoch 26/50, Train Loss: 0.6744, Val Loss: 0.6425, Accuracy: 75.99%\n",
      "Epoch 27/50, Train Loss: 0.6550, Val Loss: 0.7044, Accuracy: 69.91%\n",
      "Epoch 28/50, Train Loss: 0.6468, Val Loss: 0.7266, Accuracy: 69.91%\n",
      "Epoch 29/50, Train Loss: 0.6227, Val Loss: 0.6415, Accuracy: 73.25%\n",
      "Epoch 30/50, Train Loss: 0.6100, Val Loss: 0.6501, Accuracy: 74.47%\n",
      "Epoch 31/50, Train Loss: 0.5867, Val Loss: 0.6262, Accuracy: 77.20%\n",
      "Epoch 32/50, Train Loss: 0.5947, Val Loss: 0.6148, Accuracy: 74.16%\n",
      "Epoch 33/50, Train Loss: 0.5535, Val Loss: 0.6030, Accuracy: 75.68%\n",
      "Epoch 34/50, Train Loss: 0.5788, Val Loss: 0.6349, Accuracy: 75.08%\n",
      "Epoch 35/50, Train Loss: 0.5438, Val Loss: 0.5918, Accuracy: 79.94%\n",
      "Epoch 36/50, Train Loss: 0.5614, Val Loss: 0.5547, Accuracy: 75.08%\n",
      "Epoch 37/50, Train Loss: 0.5201, Val Loss: 0.6162, Accuracy: 77.51%\n",
      "Epoch 38/50, Train Loss: 0.5500, Val Loss: 0.6452, Accuracy: 72.64%\n",
      "Epoch 39/50, Train Loss: 0.5228, Val Loss: 0.5544, Accuracy: 79.03%\n",
      "Epoch 40/50, Train Loss: 0.5232, Val Loss: 0.5620, Accuracy: 78.72%\n",
      "Epoch 41/50, Train Loss: 0.5006, Val Loss: 0.7313, Accuracy: 68.69%\n",
      "Epoch 42/50, Train Loss: 0.5022, Val Loss: 0.5456, Accuracy: 77.81%\n",
      "Epoch 43/50, Train Loss: 0.4882, Val Loss: 0.5528, Accuracy: 77.81%\n",
      "Epoch 44/50, Train Loss: 0.4881, Val Loss: 0.7645, Accuracy: 71.73%\n",
      "Epoch 45/50, Train Loss: 0.5400, Val Loss: 0.5287, Accuracy: 77.81%\n",
      "Epoch 46/50, Train Loss: 0.4811, Val Loss: 0.4931, Accuracy: 80.85%\n",
      "Epoch 47/50, Train Loss: 0.4819, Val Loss: 0.6477, Accuracy: 74.47%\n",
      "Epoch 48/50, Train Loss: 0.4685, Val Loss: 0.5673, Accuracy: 75.99%\n",
      "Epoch 49/50, Train Loss: 0.4745, Val Loss: 0.5042, Accuracy: 79.03%\n",
      "Epoch 50/50, Train Loss: 0.4507, Val Loss: 0.5358, Accuracy: 77.51%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for back_idx, (data, label) in enumerate(train_dataloader):\n",
    "        # print(f'data loader data shape: {data.shape}')\n",
    "        #zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        #take the output\n",
    "        data = data.unsqueeze(1)\n",
    "        output = model2(data)\n",
    "        # output = model2(data)\n",
    "        #find the loss\n",
    "        loss = criterion(output, label)\n",
    "        #back propagation\n",
    "        loss.backward()\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss/len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    # break\n",
    "\n",
    "\n",
    "    model2.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.unsqueeze(1)\n",
    "            output = model2(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            val_loss += criterion(output, target.long()).item()\n",
    "        avg_val_loss = val_loss / len(test_dataloader)\n",
    "        test_losses.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {100*correct/total:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels=1, patch_size_w=50, patch_size_h=56, emb_size=50 * 56, img_size=50 * 56):\n",
    "        self.patch_size_w = patch_size_w\n",
    "        self.patch_size_h = patch_size_h\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=(patch_size_w, patch_size_h),\n",
    "                      stride=(patch_size_w, patch_size_h)),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.position = nn.Parameter(torch.randn((img_size // (patch_size_w * patch_size_h)) + 1, emb_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.position\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size=900, num_heads=5, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion=4, drop_p=0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size=900,\n",
    "                 drop_p=0.,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=1, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=900, n_classes=7):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes))\n",
    "\n",
    "class UT_HAR_ViT(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 patch_size_w=50,\n",
    "                 patch_size_h=56,\n",
    "                 emb_size=50 * 56,\n",
    "                 img_size=50 * 56,\n",
    "                 depth=1,\n",
    "                 n_classes=7,\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size_w, patch_size_h, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = UT_HAR_ViT(patch_size_w=50, patch_size_h=56, emb_size=50 * 56, img_size=50 * 56).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 2.1193, Val Loss: 2.1800, Accuracy: 18.54%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#optimizer step\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     22\u001b[0m training_losses\u001b[38;5;241m.\u001b[39mappend(avg_train_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model3.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for back_idx, (data, label) in enumerate(train_dataloader):\n",
    "        # print(f'data loader data shape: {data.shape}')\n",
    "        #zero grad the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        #take the output\n",
    "        # data = data.unsqueeze(1)\n",
    "        output = model3(data)\n",
    "        # output = model2(data)\n",
    "        #find the loss\n",
    "        loss = criterion(output, label)\n",
    "        #back propagation\n",
    "        loss.backward()\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    avg_train_loss = running_loss/len(train_dataloader)\n",
    "    training_losses.append(avg_train_loss)\n",
    "    # break\n",
    "\n",
    "\n",
    "    model3.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # data = data.unsqueeze(1)\n",
    "            output = model3(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            val_loss += criterion(output, target.long()).item()\n",
    "        avg_val_loss = val_loss / len(test_dataloader)\n",
    "        test_losses.append(avg_val_loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {100*correct/total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
